---
phase: 05-metrics-analysis-report
plan: 01
type: execute
depends_on: []
files_modified: [src/bsa/analysis/aggregate.py, src/bsa/analysis/__init__.py]
domain: python
---

<objective>
Implement analysis aggregation module that loads experiment results and computes aggregate statistics.

Purpose: Load Parquet results files from experiments, aggregate metrics across runs/models/conditions, compute statistics (mean, std, confidence intervals), and prepare data for visualization and reporting. This is the foundation for all analysis and visualization.

Output: AnalysisAggregator class that can load results, compute aggregate statistics, and prepare data for plotting and tables.
</objective>

<execution_context>
~/.cursor/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/bsa/experiments/runner.py
@src/bsa/experiments/evaluator.py
@configs/analysis/plots.yaml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create AnalysisAggregator class</name>
  <files>src/bsa/analysis/aggregate.py, src/bsa/analysis/__init__.py</files>
  <action>
    Create `src/bsa/analysis/aggregate.py` with `AnalysisAggregator` class that:
    
    1. Implements `load_results()` method that:
       - Loads Parquet files from `results/metrics/` directory
       - Can load single experiment or multiple experiments
       - Returns pandas DataFrame with all results
       - Handles missing files gracefully
    
    2. Implements `aggregate_metrics()` method that:
       - Groups results by model, condition, and other dimensions
       - Computes statistics for each metric:
         - Mean, std, min, max
         - Confidence intervals (95% CI using t-distribution)
         - Count of valid observations
       - Returns aggregated DataFrame
    
    3. Implements `compute_summary_statistics()` method that:
       - Computes overall statistics across all experiments
       - Model comparison statistics (belief_pf vs reactive vs goal_only)
       - Condition comparison statistics (false_belief vs control vs seen_relocation)
       - Returns summary dictionary
    
    4. Implements `filter_results()` method that:
       - Filters results by model, condition, goal_id, etc.
       - Useful for creating subsets for analysis
    
    5. Handles missing/NaN values appropriately:
       - Skips NaN values in aggregation
       - Reports counts of valid observations
    
    Export `AnalysisAggregator` from `src/bsa/analysis/__init__.py`.
    
    Ensure aggregation is deterministic and handles edge cases (empty results, single run, etc.).
  </action>
  <verify>
    Run `python -c "from src.bsa.analysis import AnalysisAggregator; print('Import OK')"` - should import successfully.
    Run `python -c "from src.bsa.analysis.aggregate import AnalysisAggregator; agg = AnalysisAggregator(); df = agg.load_results('results/metrics/main_experiment/results.parquet'); print(f'Loaded {len(df)} rows')"` - should load results successfully.
  </verify>
  <done>AnalysisAggregator exists, can load results, can aggregate metrics, computes statistics</done>
</task>

<task type="auto">
  <name>Task 2: Implement aggregate_results() function</name>
  <files>src/bsa/analysis/aggregate.py</files>
  <action>
    Implement `aggregate_results()` function (called by CLI) that:
    
    1. Takes config dictionary and optional input directory
    2. Creates AnalysisAggregator instance
    3. Loads results from input directory (or default `results/metrics/`)
    4. Aggregates metrics across runs/models/conditions
    5. Saves aggregated results to output directory:
       - Aggregated Parquet file
       - Summary JSON file
    6. Returns aggregated DataFrame and summary statistics
    
    This function is called by `bsa analyze` command.
    
    Ensure it handles config from `configs/analysis/plots.yaml` structure.
  </action>
  <verify>
    Run `python -c "from src.bsa.analysis.aggregate import aggregate_results; from src.bsa.common.config import load_config; from pathlib import Path; config = load_config(Path('configs/analysis/plots.yaml')); aggregate_results(config)"` - should aggregate results successfully.
  </verify>
  <done>aggregate_results() function implemented, integrates with CLI</done>
</task>

<task type="auto">
  <name>Task 3: Add metric-specific aggregation helpers</name>
  <files>src/bsa/analysis/aggregate.py</files>
  <action>
    Add helper methods for metric-specific aggregation:
    
    1. `_aggregate_detection_metrics()`: Aggregates false-belief detection metrics
       - AUROC: mean, std, CI
       - Detection latency: mean, std, CI
       - FPR: mean, std, CI
    
    2. `_aggregate_belief_tracking_metrics()`: Aggregates belief tracking metrics
       - Goal inference accuracy: mean, std, CI
       - Cross-entropy: mean, std (if available)
       - Brier score: mean, std (if available)
    
    3. `_aggregate_task_performance_metrics()`: Aggregates task performance
       - Task completion rate: percentage, CI
       - Num steps: mean, std, CI
       - Wasted actions: mean, std, CI
       - Efficiency: mean, std, CI
    
    4. `_aggregate_intervention_metrics()`: Aggregates intervention quality
       - Over/under-correction: mean, std
       - Precision/recall: mean, std, CI
       - Num interventions: mean, std
    
    These methods help organize aggregation logic and can be reused for different analyses.
  </action>
  <verify>
    Run `python -c "from src.bsa.analysis.aggregate import AnalysisAggregator; agg = AnalysisAggregator(); df = agg.load_results('results/metrics/main_experiment/results.parquet'); agg_df = agg.aggregate_metrics(df); print(f'Aggregated to {len(agg_df)} groups')"` - should aggregate successfully.
  </verify>
  <done>Metric-specific aggregation helpers implemented</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] AnalysisAggregator can load Parquet results files
- [ ] Can aggregate metrics across runs/models/conditions
- [ ] Computes statistics (mean, std, CI) correctly
- [ ] Handles missing/NaN values appropriately
- [ ] aggregate_results() function works with config
- [ ] Results saved correctly
</verification>

<success_criteria>

- AnalysisAggregator implemented and working
- Can load and aggregate experiment results
- Computes comprehensive statistics
- Ready for Plan 05-02 (Plotting module)
  </success_criteria>

<output>
After completion, create `.planning/phases/05-metrics-analysis-report/05-01-SUMMARY.md`:

# Phase 5 Plan 1: Analysis Aggregation Summary

**Implemented analysis aggregation module for loading and aggregating experiment results**

## Accomplishments

- Created AnalysisAggregator class
- Can load Parquet results files
- Aggregates metrics across runs/models/conditions
- Computes statistics (mean, std, confidence intervals)
- Metric-specific aggregation helpers

## Files Created/Modified

- `src/bsa/analysis/aggregate.py` - AnalysisAggregator class and aggregate_results() function
- `src/bsa/analysis/__init__.py` - Export AnalysisAggregator

## Decisions Made

- Aggregation approach: Group by model/condition, compute statistics
- Statistics computed: mean, std, min, max, 95% CI
- Handling of NaN values: Skip in aggregation, report counts
- Output format: Aggregated Parquet + summary JSON

## Issues Encountered

[Any issues with loading results, aggregation, statistics computation, etc.]

## Next Step

Ready for 05-02-PLAN.md (Plotting module)
</output>
