---
phase: 01-core-interfaces-gridhouse
plan: 03
type: execute
depends_on: ["01-02"]
files_modified: [src/bsa/envs/gridhouse/episode_generator.py, src/bsa/envs/gridhouse/recorder.py]
---

<objective>
Implement episode serialization to Parquet and JSONL formats for research data storage.

Purpose: Episodes must be saved in efficient, schema-enforced formats that enable large-scale data collection and analysis. Parquet provides efficient storage, JSONL provides human-readable fallback.

Output: Episode recorder that saves episodes to Parquet (primary) and JSONL (fallback) with proper schema validation.
</objective>

<execution_context>
~/.cursor/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-interfaces-gridhouse/01-02-SUMMARY.md
@src/bsa/common/types.py
@src/bsa/envs/gridhouse/episode_generator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create episode recorder with Parquet serialization</name>
  <files>src/bsa/envs/gridhouse/recorder.py</files>
  <action>
Create EpisodeRecorder class in src/bsa/envs/gridhouse/recorder.py:
- Method `save_episode(episode: Episode, output_path: Path, format: str = 'parquet') -> None`
- Convert Episode dataclass to pandas DataFrame:
  - Each EpisodeStep becomes a row
  - Flatten nested structures (ObjectLocation -> columns: object_id, container_id, room_id, x, y, z)
  - Convert Action enum to string
  - Convert Observation to columns (visible_objects as list, current_room, position as x/y/z)
- Save to Parquet using pyarrow:
  - Define schema explicitly (string, int, float, list types)
  - Use compression (snappy or gzip)
  - Include episode metadata as Parquet metadata
- Handle multiple episodes: append mode for batch saving

Schema should include:
- episode_id, timestep, goal_id, tau, intervention_type
- human_action (string), human_obs_*, helper_obs_*
- true_object_locations_* (flattened), human_belief_object_locations_* (flattened)
- visible_objects_h, visible_objects_helper (as lists)

Use pyarrow for Parquet, pandas for DataFrame manipulation. Add type hints.
</action>
  <verify>python -c "from src.bsa.envs.gridhouse import GridHouseEnvironment, GridHouseEpisodeGenerator; from src.bsa.envs.gridhouse.recorder import EpisodeRecorder; from pathlib import Path; import os; env = GridHouseEnvironment(seed=42); gen = GridHouseEpisodeGenerator(env, seed=42); ep = gen.generate_episode(); recorder = EpisodeRecorder(); output = Path('test_episode.parquet'); recorder.save_episode(ep, output); assert output.exists(), 'Parquet file not created'; os.remove(output)"</verify>
  <done>EpisodeRecorder saves episodes to Parquet with proper schema, can be read back</done>
</task>

<task type="auto">
  <name>Task 2: Add JSONL fallback format</name>
  <files>src/bsa/envs/gridhouse/recorder.py</files>
  <action>
Add JSONL serialization to EpisodeRecorder:
- Method `save_episode_jsonl(episode: Episode, output_path: Path) -> None`
- Convert Episode to JSON-serializable format:
  - Convert Action enum to string
  - Convert ObjectLocation to dict
  - Convert Observation to dict
  - Handle nested structures (lists, dicts)
- Write one JSON object per line (JSONL format)
- Each line is a complete EpisodeStep serialized as JSON
- Include episode metadata as first line or separate metadata file

Ensure JSONL format:
- Human-readable for debugging
- Can be loaded line-by-line for streaming
- Compatible with Episode dataclass structure
- Handles all data types correctly (no serialization errors)

Add format parameter to save_episode: 'parquet' (default) or 'jsonl'. Use json module for JSONL.
</action>
  <verify>python -c "from src.bsa.envs.gridhouse import GridHouseEnvironment, GridHouseEpisodeGenerator; from src.bsa.envs.gridhouse.recorder import EpisodeRecorder; from pathlib import Path; import json, os; env = GridHouseEnvironment(seed=42); gen = GridHouseEpisodeGenerator(env, seed=42); ep = gen.generate_episode(); recorder = EpisodeRecorder(); output = Path('test_episode.jsonl'); recorder.save_episode(ep, output, format='jsonl'); assert output.exists(), 'JSONL file not created'; with open(output) as f: line = json.loads(f.readline()); assert 'episode_id' in line, 'Invalid JSONL format'; os.remove(output)"</verify>
  <done>JSONL serialization works, episodes can be saved in both formats</done>
</task>

<task type="auto">
  <name>Task 3: Integrate recorder into episode generator and add batch generation</name>
  <files>src/bsa/envs/gridhouse/episode_generator.py</files>
  <action>
Update GridHouseEpisodeGenerator to support batch generation and saving:
- Add method `generate_episodes(num_episodes: int, output_dir: Path, format: str = 'parquet') -> List[Episode]`
- Generate multiple episodes with configurable distributions:
  - Goal distribution (from config or uniform)
  - Tau distribution (from tau_range)
  - Intervention type distribution (drift_probability)
- Save each episode using EpisodeRecorder
- Return list of generated episodes
- Add progress logging (tqdm or print statements)

Update generate_episode to optionally save immediately:
- Add optional save_path parameter
- If provided, save episode after generation
- Otherwise, return episode object

Ensure:
- Episodes have unique IDs
- Output directory created if needed
- File naming: `episode_{episode_id}.{ext}`
- Batch generation is deterministic (seed handling)

Import EpisodeRecorder and use it for saving.
</action>
  <verify>python -c "from src.bsa.envs.gridhouse import GridHouseEnvironment, GridHouseEpisodeGenerator; from pathlib import Path; import os, shutil; env = GridHouseEnvironment(seed=42); gen = GridHouseEpisodeGenerator(env, seed=42); output_dir = Path('test_episodes'); gen.generate_episodes(5, output_dir, format='parquet'); files = list(output_dir.glob('*.parquet')); assert len(files) == 5, f'Expected 5 files, got {len(files)}'; shutil.rmtree(output_dir)"</verify>
  <done>Batch episode generation works, episodes saved to specified directory in requested format</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Episodes can be saved to Parquet format
- [ ] Episodes can be saved to JSONL format
- [ ] Saved episodes can be loaded and validated
- [ ] Batch generation produces multiple episodes
- [ ] File naming and directory structure correct
- [ ] No data loss in serialization
</verification>

<success_criteria>

- EpisodeRecorder class implemented with Parquet and JSONL support
- Episodes serialize correctly with proper schema
- Batch generation works with configurable distributions
- Saved episodes can be loaded and validated
- All data types handled correctly (enums, nested structures)

</success_criteria>

<output>
After completion, create `.planning/phases/01-core-interfaces-gridhouse/01-03-SUMMARY.md`:

# Phase 1 Plan 3: Episode Serialization Summary

**Implemented episode serialization to Parquet and JSONL formats**

## Accomplishments

- Created EpisodeRecorder class with Parquet serialization
- Added JSONL fallback format support
- Integrated recorder into episode generator with batch generation
- Episodes can be saved and loaded for research data collection

## Files Created/Modified

- `src/bsa/envs/gridhouse/recorder.py` - EpisodeRecorder class
- `src/bsa/envs/gridhouse/episode_generator.py` - Batch generation and saving integration

## Decisions Made

- Parquet as primary format (efficient, schema-enforced)
- JSONL as fallback (human-readable, streaming-friendly)
- Episode ID used for file naming
- Batch generation supports configurable distributions

## Issues Encountered

None

## Next Step

Phase 1 complete. Ready for Phase 2 (Helper Models).

</output>
