---
phase: 04-experiment-harness
plan: 02
type: execute
depends_on: ["04-01"]
files_modified: [src/bsa/experiments/evaluator.py, src/bsa/experiments/runner.py, src/bsa/metrics/__init__.py]
domain: python
---

<objective>
Create comprehensive episode evaluator with detailed metrics computation.

Purpose: Enhance episode evaluation with comprehensive metrics including false-belief detection (AUROC, detection latency), belief tracking accuracy, task performance, and intervention quality. This enables detailed analysis of helper agent performance.

Output: EpisodeEvaluator class with comprehensive metrics computation, integration with ExperimentRunner.
</objective>

<execution_context>
~/.cursor/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-experiment-harness/04-01-SUMMARY.md
@src/bsa/experiments/runner.py
@src/bsa/common/types.py
@src/bsa/agents/helper/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create EpisodeEvaluator class with comprehensive metrics</name>
  <files>src/bsa/experiments/evaluator.py, src/bsa/experiments/__init__.py</files>
  <action>
    Create `src/bsa/experiments/evaluator.py` with `EpisodeEvaluator` class that:
    
    1. Implements `evaluate_episode()` method that evaluates a single episode with a helper agent:
       - Runs helper agent through episode step-by-step
       - Collects detailed metrics at each step
       - Computes episode-level metrics
    
    2. Implements metrics computation methods:
       - `_compute_false_belief_detection_metrics()`: AUROC, detection latency, FPR
         - Compares helper's false-belief predictions vs. ground truth
         - Computes detection latency (timestep when false belief detected)
         - Computes false positive rate
       
       - `_compute_belief_tracking_metrics()`: Accuracy, cross-entropy, Brier score
         - Compares helper's belief distributions vs. true state
         - Computes accuracy for goal inference
         - Computes cross-entropy for belief distributions
         - Computes Brier score for probabilistic predictions
       
       - `_compute_task_performance_metrics()`: Completion, wasted actions, efficiency
         - Checks if task was completed
         - Counts wasted actions (actions that don't progress toward goal)
         - Computes efficiency (useful actions / total actions)
       
       - `_compute_intervention_metrics()`: Over/under-correction, precision/recall
         - Tracks when helper intervenes
         - Compares interventions to ground truth need
         - Computes over-correction (intervened when not needed)
         - Computes under-correction (didn't intervene when needed)
         - Computes precision and recall for interventions
    
    3. Returns comprehensive metrics dictionary with all computed metrics
    
    Export `EpisodeEvaluator` from `src/bsa/experiments/__init__.py`.
    
    Ensure metrics computation is deterministic and handles edge cases (no false beliefs, no interventions, etc.).
  </action>
  <verify>
    Run `python -c "from src.bsa.experiments import EpisodeEvaluator; print('Import OK')"` - should import successfully.
    Run `python -c "from src.bsa.experiments.evaluator import EpisodeEvaluator; from src.bsa.envs.gridhouse import GridHouseEnvironment, GridHouseEpisodeGenerator; from src.bsa.agents.helper import BeliefSensitiveHelper; env = GridHouseEnvironment(seed=42); gen = GridHouseEpisodeGenerator(env, seed=42); episode = gen.generate_episode(goal_id='prepare_meal', tau=5, intervention_type='relocate'); helper = BeliefSensitiveHelper(num_particles=10); evaluator = EpisodeEvaluator(); metrics = evaluator.evaluate_episode(episode, helper); print(f'Metrics keys: {list(metrics.keys())}')"` - should evaluate episode and return comprehensive metrics.
  </verify>
  <done>EpisodeEvaluator exists, computes all required metrics (false-belief detection, belief tracking, task performance, intervention quality)</done>
</task>

<task type="auto">
  <name>Task 2: Integrate EpisodeEvaluator with ExperimentRunner</name>
  <files>src/bsa/experiments/runner.py</files>
  <action>
    Update `ExperimentRunner` to use `EpisodeEvaluator`:
    
    1. Replace `_evaluate_episode()` method to use `EpisodeEvaluator.evaluate_episode()`
    2. Ensure ExperimentRunner collects all metrics from EpisodeEvaluator
    3. Update result saving to include all computed metrics
    
    Maintain backward compatibility - ExperimentRunner should still work if EpisodeEvaluator is not available (graceful fallback).
  </action>
  <verify>
    Run `python -c "from src.bsa.experiments import ExperimentRunner; runner = ExperimentRunner({'models': ['reactive'], 'conditions': ['control'], 'num_runs': 1, 'seed': 42}); print('Runner uses EpisodeEvaluator')"` - should work without errors.
  </verify>
  <done>ExperimentRunner uses EpisodeEvaluator, all metrics collected and saved</done>
</task>

<task type="auto">
  <name>Task 3: Create metrics module structure</name>
  <files>src/bsa/metrics/__init__.py</files>
  <action>
    Create `src/bsa/metrics/__init__.py` to export metric computation utilities:
    
    - Export helper functions for metric computation (if any)
    - Prepare structure for future metric modules (drift_detection.py, belief_tracking.py, etc.)
    
    This sets up the structure for Phase 5 (Metrics + Analysis).
  </action>
  <verify>
    Run `python -c "from src.bsa.metrics import *; print('Metrics module OK')"` - should import without errors.
  </verify>
  <done>Metrics module structure created, ready for Phase 5</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] EpisodeEvaluator computes all required metrics
- [ ] False-belief detection metrics (AUROC, latency, FPR) work
- [ ] Belief tracking metrics (accuracy, cross-entropy, Brier) work
- [ ] Task performance metrics (completion, wasted actions) work
- [ ] Intervention metrics (over/under-correction, precision/recall) work
- [ ] ExperimentRunner integrates with EpisodeEvaluator
- [ ] Metrics module structure exists
</verification>

<success_criteria>

- EpisodeEvaluator implemented with comprehensive metrics
- All metrics computed correctly
- ExperimentRunner integrated with EpisodeEvaluator
- Metrics module structure ready for Phase 5
- Ready for Plan 04-03 (Sweep runner and ablations)
  </success_criteria>

<output>
After completion, create `.planning/phases/04-experiment-harness/04-02-SUMMARY.md`:

# Phase 4 Plan 2: Episode Evaluator Summary

**Implemented comprehensive episode evaluator with detailed metrics**

## Accomplishments

- Created EpisodeEvaluator class with comprehensive metrics computation
- Implemented false-belief detection metrics (AUROC, latency, FPR)
- Implemented belief tracking metrics (accuracy, cross-entropy, Brier)
- Implemented task performance metrics (completion, wasted actions)
- Implemented intervention metrics (over/under-correction, precision/recall)
- Integrated with ExperimentRunner

## Files Created/Modified

- `src/bsa/experiments/evaluator.py` - EpisodeEvaluator class
- `src/bsa/experiments/runner.py` - Updated to use EpisodeEvaluator
- `src/bsa/experiments/__init__.py` - Export EpisodeEvaluator
- `src/bsa/metrics/__init__.py` - Metrics module structure

## Decisions Made

- Metrics computation approach (step-by-step vs. episode-level)
- How to compute AUROC for false-belief detection
- How to measure detection latency
- How to compute over/under-correction
- Metrics storage format (dictionary structure)

## Issues Encountered

[Any issues with metrics computation, edge cases, etc.]

## Next Step

Ready for 04-03-PLAN.md (Sweep runner and ablations)
</output>
