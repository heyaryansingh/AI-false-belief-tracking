---
phase: 06-tests-ci
plan: 03
type: execute
depends_on: ["06-02"]
files_modified: [tests/test_experiments.py, tests/test_analysis.py]
domain: python
---

<objective>
Create comprehensive unit tests for experiment and analysis components.

Purpose: Ensure experiment runner, evaluator, sweep runner, analysis aggregation, plotting, tables, and report generation work correctly. These tests verify the full pipeline components work as expected.

Output: Test files covering experiment runner, evaluator, analysis aggregation, plotting, tables, and report generation.
</objective>

<execution_context>
~/.cursor/get-shit-done/workflows/execute-plan.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-tests-ci/06-02-SUMMARY.md
@src/bsa/experiments/runner.py
@src/bsa/experiments/evaluator.py
@src/bsa/experiments/sweep.py
@src/bsa/analysis/aggregate.py
@src/bsa/viz/plots.py
@src/bsa/analysis/tables.py
@src/bsa/analysis/report.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create experiment component tests</name>
  <files>tests/test_experiments.py</files>
  <action>
    Create `tests/test_experiments.py` with tests for experiment components:
    
    1. **ExperimentRunner tests**:
       - `test_initialization()`: Test runner initializes with config
       - `test_create_helper_agent()`: Test helper agent creation (reactive, goal_only, belief_pf)
       - `test_get_episode_generator()`: Test episode generator selection (GridHouse/VirtualHome)
       - `test_run_experiment()`: Test running single experiment
       - `test_save_results()`: Test saving results to Parquet
       - `test_multiple_models()`: Test running experiments with multiple models
       - `test_multiple_conditions()`: Test running experiments with multiple conditions
    
    2. **EpisodeEvaluator tests**:
       - `test_initialization()`: Test evaluator initializes
       - `test_evaluate_episode()`: Test episode evaluation
       - `test_false_belief_detection_metrics()`: Test detection metrics computation
       - `test_belief_tracking_metrics()`: Test belief tracking metrics
       - `test_task_performance_metrics()`: Test task performance metrics
       - `test_intervention_metrics()`: Test intervention quality metrics
       - `test_comprehensive_metrics()`: Test all metrics computed correctly
    
    3. **SweepRunner tests**:
       - `test_initialization()`: Test sweep runner initializes
       - `test_parameter_sweep()`: Test parameter sweep execution
       - `test_sweep_results()`: Test sweep results saving
    
    4. Use pytest fixtures for:
       - Sample experiment configs
       - Sample episodes
       - Sample helper agents
    
    5. Use temporary directories for output files.
  </action>
  <verify>
    Run `pytest tests/test_experiments.py -v` - should run all tests successfully.
    Check test coverage includes all experiment components.
  </verify>
  <done>Experiment component tests implemented, covering runner, evaluator, and sweep runner</done>
</task>

<task type="auto">
  <name>Task 2: Create analysis component tests</name>
  <files>tests/test_analysis.py</files>
  <action>
    Create `tests/test_analysis.py` with tests for analysis components:
    
    1. **AnalysisAggregator tests**:
       - `test_load_results()`: Test loading Parquet files
       - `test_aggregate_metrics()`: Test metric aggregation
       - `test_compute_statistics()`: Test statistics computation (mean, std, CI)
       - `test_filter_results()`: Test result filtering
       - `test_compute_summary_statistics()`: Test summary statistics
    
    2. **PlotGenerator tests**:
       - `test_initialization()`: Test plot generator initializes
       - `test_plot_detection_auroc()`: Test AUROC plot generation
       - `test_plot_task_performance()`: Test task performance plot generation
       - `test_plot_intervention_quality()`: Test intervention quality plot generation
       - `test_plot_belief_timeline()`: Test belief timeline plot generation
       - `test_generate_plots()`: Test generate_plots() function
       - Verify plots are saved correctly
    
    3. **TableGenerator tests**:
       - `test_initialization()`: Test table generator initializes
       - `test_generate_summary_table()`: Test summary table generation
       - `test_generate_detection_table()`: Test detection table generation
       - `test_generate_task_performance_table()`: Test task performance table generation
       - `test_generate_intervention_table()`: Test intervention table generation
       - `test_table_formats()`: Test Markdown and LaTeX formats
       - `test_generate_tables()`: Test generate_tables() function
    
    4. **ReportGenerator tests**:
       - `test_initialization()`: Test report generator initializes
       - `test_generate_report()`: Test report generation
       - `test_placeholder_filling()`: Test all placeholders are filled
       - `test_report_structure()`: Test report has correct structure
       - `test_figure_inclusion()`: Test figures are included correctly
       - `test_table_inclusion()`: Test tables are included correctly
    
    5. Use pytest fixtures for:
       - Sample aggregated results DataFrames
       - Sample summary statistics
       - Sample figure paths
       - Sample table paths
    
    6. Use temporary directories for output files.
  </action>
  <verify>
    Run `pytest tests/test_analysis.py -v` - should run all tests successfully.
    Check test coverage includes all analysis components.
  </verify>
  <done>Analysis component tests implemented, covering aggregation, plotting, tables, and report generation</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] Experiment component tests pass
- [ ] Analysis component tests pass
- [ ] Tests verify correct metric computation
- [ ] Tests verify correct plot/table/report generation
- [ ] Test coverage >70% for tested modules
</verification>

<success_criteria>

- Comprehensive unit tests for experiment and analysis components
- Tests verify correct functionality and output generation
- Ready for Plan 06-04 (Integration tests)
  </success_criteria>

<output>
After completion, create `.planning/phases/06-tests-ci/06-03-SUMMARY.md`:

# Phase 6 Plan 3: Experiment and Analysis Component Tests Summary

**Implemented comprehensive unit tests for experiment and analysis components**

## Accomplishments

- Created tests for ExperimentRunner, EpisodeEvaluator, SweepRunner
- Created tests for AnalysisAggregator, PlotGenerator, TableGenerator, ReportGenerator
- Tests verify correct metric computation and output generation

## Files Created/Modified

- `tests/test_experiments.py` - Experiment component tests
- `tests/test_analysis.py` - Analysis component tests

## Decisions Made

- Test structure: Separate test classes for each component
- Output testing: Verify files are generated correctly
- Metric testing: Verify metrics are computed correctly
- Temporary directories: Use for test outputs

## Issues Encountered

[Any issues with test implementation, metric computation, etc.]

## Next Step

Ready for 06-04-PLAN.md (Integration tests)
</output>
